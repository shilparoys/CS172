Partner 1: Katherine Gallaher kgall005@ucr.edu
Partner 2: Shilpa Chirackel   schir001@ucr.edu

Collobration Details:
We divided up the work evenly. When one partner got stuck debugging, the other partner would step in to try to fix the problem. 
The specifics are as follows:
Shilpa Chirckel: Shilpa mainly worked on downloading the seed from a text file, parsing the html file, and cleaning up the URLs.  
Katherine Gallaher: Katherine mainly worked on getting the web crawler to crawl x amount of pages and y amount of hops, threading, and avoiding urls that we cannot crawl based on robots.txt
As stated before, the items mentioned above are no limitied to the particular parnter. Each individual worked on each aspect of the code one way or another not limited to implementation strategies, implementation, and debugging. 

Overview of The System
Architecture
The Crawling or Data Collection Strategy
Data Structures Employed

Limitations (if any)
We tried to make our program as roboust as possible. We have put necessary condiitions to make sure that user does not enter malformed data and files. We also added code that checks that the URLs that are passed in are valid. To improve effiecieny, we implemented our program using threads. This allows our crawler to crawl multiple sites at the same time. 

Instructions on how to deploy system
Please take a look at the ReadMe document to check how to deploy the system. We created a shell script to run our program. 

Screenshot of System at action 
